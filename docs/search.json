[
  {
    "objectID": "Fractal_LLM_Course/lesson_1/summary.html",
    "href": "Fractal_LLM_Course/lesson_1/summary.html",
    "title": "",
    "section": "",
    "text": "Intro Slides:\nUnderstanding LLMs: - LLMs have revolutionized the NLP field. - Access these powerful models with HuggingFace‚Äôs API.\nStarting with Sentiment Analysis: - Consider: How do sentiment analyses of user reviews shape product development?\nIntroduction to HuggingFace: - Dive into a concise overview of the HuggingFace API. - Discover the pipeline‚Äôs simplicity and efficiency ‚Äì all in just ‚Äú3 lines of code.‚Äù\nUsing Jupyter Notebooks:\nSetting up the Notebook: - Enhance your notebook with extensions like autoreload. - Grasp the importance of an editable python install (pip install -e .).\nDeveloping a Sentiment Analysis pipeline: - Kickstart your pipeline with the HuggingFace transformers library. - For guidance: from transformers import pipeline - Learn which model the pipeline defaults to: distilbert. - Chart your course to set up the classifier for Sentiment Analysis.\nUnderstanding the classifier: - Navigate the classifier using Jupyter‚Äôs commands (?, ??, help()). - Your toolkit: classifier?, classifier??, and help(classifier). - Decode the classifier‚Äôs design and function for better insights. &gt; Note: Adopt this exploration strategy for any python object.\nExamining the pipeline: - Dissect the heart of the pipeline: - Config - Tokenizer - Model - Unpack each component with the DistilBert model as your guide. - Construct a simple_pipeline and master its steps: - Tokenization - Channeling tokens to the model - Drawing meaning from the results.\nUsing Auto* Classes: - Step into the realm of HuggingFace‚Äôs Auto* classes for Config, Tokenizer, and Model. - Expedite your model experiments leveraging these classes.\nInsights on the pipeline: - Unlock advanced LLM features with ease. - Harness your newfound knowledge to design and hone NLP pipelines.\nConnecting Components: - Unite Config, Tokenizer, Model, and their respective repository files. - For clarity, check out the distilbert repository. - Appreciate the pivotal role these components play in numerous HuggingFace NLP models. &gt; Point: Traverse deeper, and you might find more classes tailored for specialized NLP models or other domains.\nComparing Manual vs.¬†pipeline Approaches: - Weigh the strengths and weaknesses of both avenues. - Choose wisely based on your needs.\nTasks to Consider: - Challenge: Embark on sentiment analysis with a fresh dataset. &gt; Tip: Delve into the HuggingFace datasets module for a head start. - Test your skills with another pipeline feat, be it Text Summarization or Text Generation."
  },
  {
    "objectID": "nbs/00_setup.html",
    "href": "nbs/00_setup.html",
    "title": "Environment setups",
    "section": "",
    "text": "Running a Large Language Model using the HuggingFace Transformers API."
  },
  {
    "objectID": "nbs/00_setup.html#things-we-need-for-the-class",
    "href": "nbs/00_setup.html#things-we-need-for-the-class",
    "title": "Environment setups",
    "section": "Things we need for the class",
    "text": "Things we need for the class\nIn order to fully use a current, open source LLM, the first thing we need to do is set up a proper programming environment. The environment is a computing ecosystem with all the software libraries and packages needed to run an LLM.\nNote that setting up this environment is often one of the most time-consuming and challenging tasks when it comes to Machine Learning. There is no silver bullet or universal solution, as you will see by the dozens of tools that folks have come up with to tackle this problem (insert xkcd comic about competing standards).\nThe main point here is that setting up the environment is often annoying. It can even be straight up painful. It‚Äôs ok to feel lost or struggle with it. Please take some comfort in the fact that once we have the environment, many of the downstream tasks will feel easy by comparison!\nSo what makes building this environment so challenging? And why do we need it in the first place?\n\nSilent Failures in AI Models\nLLMs, and Machine Learning models more generally, often fail in different ways than other, standard software fails. For instance, classic bugs in regular software include: type mismatches, syntax errors, compilation errors, etc. In other words failures that clearly stem from a wrong operation (aka a bug) that snuck into the code. We wanted the computer to do X, but we told it by accident to do Y instead.\nIn contrast, ML models often have ‚Äúsilent‚Äù failures. There is no syntax or compilation error - the program still runs and completes fine. But, there is something wrong in the code: adding where we should have subtracted, grabbing the wrong element from a list, or using the wrong mathematical function. There is no type checker or compiler that would (or even could, for now) catch these errors.\nThe fix for the silent failures above is clear:\n- Carefully inspecting the code.\n- Monitoring and validating its outputs.\n- Clarity in both the algorithms and models we are using.\nThere is another, unfortunate kind of silent failure: version mismatches. Version failures happen when we use a different version of a programming library than the version originally used by the model. As the software libraries we rely on are frequently updated, both subtle and major changes in their internals can affect the output of a model. These failures are unfortunately immune to our careful logical checks.\nAvoiding these silent failures is the main reason for being consistent and disciplined with our model‚Äôs programming environment. A good environment setup keeps us focused on the important, conceptual part of our model instead of getting bogged down in managing software versions.\n\n\nLooking forward with our environment\nThere is a nice benefit to spending this much time and effort up front on our environment.\nWe will not only have a specialized environment to run and fine-tune a single LLM. We‚Äôll have a springboard and setup to keep up with the state of the art in the field. A setup to bring in groundbreaking improvements as they are released. And to weave in the latest and greatest models. The LLM world is our oyster, and the base environment the grain of sand soon-to-be pearls."
  },
  {
    "objectID": "nbs/00_setup.html#organizing-what-we-need",
    "href": "nbs/00_setup.html#organizing-what-we-need",
    "title": "Environment setups",
    "section": "Organizing what we need",
    "text": "Organizing what we need\nThe mamba package manager will handle the python version. Why Mamba? To start it is way fast and better than Anaconda, and it makes it easier to install OS and system-level packages we need outside of python.\nWe will use pip to install the actual python packages. Note that we could use mamba for this as well, but a few of the libraries need custom pip options to install.\n\nNote: Run pip install -e . to install a dynamic version of this package that tracks live code changes."
  },
  {
    "objectID": "nbs/00_setup.html#mac-installation",
    "href": "nbs/00_setup.html#mac-installation",
    "title": "Environment setups",
    "section": "Mac Installation",
    "text": "Mac Installation\nFirst find the name of your architecture. We then use it to pick the right install script for each Mac.\n\n# check your mac's architecture\narch=$(uname) \necho $arch\n\n# download the appropriate installation script\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\"\n\n# run the Mambaforge installer\nbash Mambaforge-$(uname)-$(uname -m).sh\nIf you prefer to download the file directly, grab it from here:\nhttps://github.com/conda-forge/miniforge/releases/"
  },
  {
    "objectID": "nbs/00_setup.html#creating-the-environment",
    "href": "nbs/00_setup.html#creating-the-environment",
    "title": "Environment setups",
    "section": "Creating the environment",
    "text": "Creating the environment\nAfter installing Mamba, head to the Lesson 0 here: Fractal_LLM_Course/lesson_0/envs. The README.md in that folder has the full instructions to build the mamba environment."
  },
  {
    "objectID": "nbs/02_nbdev.html",
    "href": "nbs/02_nbdev.html",
    "title": "Publishing blog posts directly from Notebooks",
    "section": "",
    "text": "Publish your first blog post.\n\nCreate a full, proper python library for the custom Sentiment Analysis pipeline from last time.\n\nHere we take a closer look at the nbdev library. nbdev is a powerful tool based on two key ideas:\n- Literate programming. - Exploratory programming.\nIn Literate Programming, descriptions (documentation) are woven directly into a project‚Äôs source code.\nThis is opposite of how most codebases are set up, where documentation exists as a separate set of files.\nHow is this different from parsing source code and comments into documentation? In Literate Programming the code, tests, and documentation are all first-class citizens. And with nbdev, the Notebook is the single source of truth for all of these. Think about what this truly means, and just how much overhead work it removes.\nInstead of having to independently manage code, docs, and tests, everything can be done in Notebooks. If the notebook runs, you know your code will run. And you can directly document and test your code in the notebook as you develop.\nExploratory programming is, as the name suggests, an open-ended approach to coding when exploring unknown domains or areas. Folks often use it at the start of a project, when the requirements or scope are not yet flushed out.\nTry things out, figure out how they work, what they do. Poke around. Explore. Follow that curiosity. Have fun! Can always restart the Notebook, no fear around trying things - very little to no downside.\nnbdev combines these two ideas. We can then mix and match them in different doses as needed.\nThis notebook will turn our previous, first runs into a proper python library.\nIt will have documentation and tests directly in the notebook.\n\n%load_ext autoreload\n%autoreload 2\n\n::: {.cell 0=‚Äòe‚Äô 1=‚Äòx‚Äô 2=‚Äòp‚Äô 3=‚Äòo‚Äô 4=‚Äòr‚Äô 5=‚Äòt‚Äô}\nfrom transformers import AutoConfig\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification\n:::\n::: {.cell 0=‚Äòe‚Äô 1=‚Äòx‚Äô 2=‚Äòp‚Äô 3=‚Äòo‚Äô 4=‚Äòr‚Äô 5=‚Äòt‚Äô}\nclass SentimentPipeline:\n    def __init__(self, model_name):\n        \"\"\"\n        Sentiment Analysis pipeline.\n        \"\"\"\n        self.model_name = model_name\n        self.config = AutoConfig.from_pretrained(self.model_name)\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)\n\n\n    def preprocess(self, text: str):\n        \"\"\"\n        Sends `text` through the LLM's tokenizer.  \n        The tokenizers turns words and characters into special inputs for the LLM.\n        \"\"\"\n        tokenized_inputs = self.tokenizer(text, return_tensors='pt')\n        return tokenized_inputs\n    \n\n    def forward(self, text: str):\n        \"\"\"\n        First we preprocess the `text` into tokens.\n        Then we send the `token_inputs` to the model.\n        \"\"\"\n        token_inputs = self.preprocess(text)\n        outputs = self.model(**token_inputs)\n        return outputs\n    \n\n    def process_outputs(self, outs):\n        \"\"\"\n        Here is where HuggingFace does the most for us via `pipeline`.  \n        \"\"\"\n        # grab the raw \"scores\" that from the model for Positive and Negative labels\n        logits = outs.logits\n\n        # find the strongest label score, aka the model's decision\n        pred_idx = logits.argmax(1).item()\n\n        # use the `config` object to find the class label\n        pred_label = self.config.id2label[pred_idx]  \n\n        # calculate the human-readable number for the score\n        pred_score = logits.softmax(-1)[:, pred_idx].item()\n\n        return {\n            'label': pred_label,\n            'score': pred_score, \n        }\n    \n    def __call__(self, text: str):\n        model_outs = self.forward(text)\n        preds = self.process_outputs(model_outs)\n        return preds\n    \n    def __repr__(self):\n        return f\"SentimentAnalysis_{self.model_name}\"\n\n:::"
  },
  {
    "objectID": "nbs/02_nbdev.html#goals",
    "href": "nbs/02_nbdev.html#goals",
    "title": "Publishing blog posts directly from Notebooks",
    "section": "",
    "text": "Publish your first blog post.\n\nCreate a full, proper python library for the custom Sentiment Analysis pipeline from last time.\n\nHere we take a closer look at the nbdev library. nbdev is a powerful tool based on two key ideas:\n- Literate programming. - Exploratory programming.\nIn Literate Programming, descriptions (documentation) are woven directly into a project‚Äôs source code.\nThis is opposite of how most codebases are set up, where documentation exists as a separate set of files.\nHow is this different from parsing source code and comments into documentation? In Literate Programming the code, tests, and documentation are all first-class citizens. And with nbdev, the Notebook is the single source of truth for all of these. Think about what this truly means, and just how much overhead work it removes.\nInstead of having to independently manage code, docs, and tests, everything can be done in Notebooks. If the notebook runs, you know your code will run. And you can directly document and test your code in the notebook as you develop.\nExploratory programming is, as the name suggests, an open-ended approach to coding when exploring unknown domains or areas. Folks often use it at the start of a project, when the requirements or scope are not yet flushed out.\nTry things out, figure out how they work, what they do. Poke around. Explore. Follow that curiosity. Have fun! Can always restart the Notebook, no fear around trying things - very little to no downside.\nnbdev combines these two ideas. We can then mix and match them in different doses as needed.\nThis notebook will turn our previous, first runs into a proper python library.\nIt will have documentation and tests directly in the notebook.\n\n%load_ext autoreload\n%autoreload 2\n\n::: {.cell 0=‚Äòe‚Äô 1=‚Äòx‚Äô 2=‚Äòp‚Äô 3=‚Äòo‚Äô 4=‚Äòr‚Äô 5=‚Äòt‚Äô}\nfrom transformers import AutoConfig\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification\n:::\n::: {.cell 0=‚Äòe‚Äô 1=‚Äòx‚Äô 2=‚Äòp‚Äô 3=‚Äòo‚Äô 4=‚Äòr‚Äô 5=‚Äòt‚Äô}\nclass SentimentPipeline:\n    def __init__(self, model_name):\n        \"\"\"\n        Sentiment Analysis pipeline.\n        \"\"\"\n        self.model_name = model_name\n        self.config = AutoConfig.from_pretrained(self.model_name)\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)\n\n\n    def preprocess(self, text: str):\n        \"\"\"\n        Sends `text` through the LLM's tokenizer.  \n        The tokenizers turns words and characters into special inputs for the LLM.\n        \"\"\"\n        tokenized_inputs = self.tokenizer(text, return_tensors='pt')\n        return tokenized_inputs\n    \n\n    def forward(self, text: str):\n        \"\"\"\n        First we preprocess the `text` into tokens.\n        Then we send the `token_inputs` to the model.\n        \"\"\"\n        token_inputs = self.preprocess(text)\n        outputs = self.model(**token_inputs)\n        return outputs\n    \n\n    def process_outputs(self, outs):\n        \"\"\"\n        Here is where HuggingFace does the most for us via `pipeline`.  \n        \"\"\"\n        # grab the raw \"scores\" that from the model for Positive and Negative labels\n        logits = outs.logits\n\n        # find the strongest label score, aka the model's decision\n        pred_idx = logits.argmax(1).item()\n\n        # use the `config` object to find the class label\n        pred_label = self.config.id2label[pred_idx]  \n\n        # calculate the human-readable number for the score\n        pred_score = logits.softmax(-1)[:, pred_idx].item()\n\n        return {\n            'label': pred_label,\n            'score': pred_score, \n        }\n    \n    def __call__(self, text: str):\n        model_outs = self.forward(text)\n        preds = self.process_outputs(model_outs)\n        return preds\n    \n    def __repr__(self):\n        return f\"SentimentAnalysis_{self.model_name}\"\n\n:::"
  },
  {
    "objectID": "nbs/01_first_runs.html",
    "href": "nbs/01_first_runs.html",
    "title": "Run a Large Language Model using the HuggingFace Transformers API.",
    "section": "",
    "text": "The cells below are good defaults for development.\nThe autoreload lines help load libraries on the fly, while they are changing. This works well with the editable install we created via pip install -e .\nThis means we can edit the source code directly and have the change reflected live in the notebook.\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline"
  },
  {
    "objectID": "nbs/01_first_runs.html#first-a-pipeline",
    "href": "nbs/01_first_runs.html#first-a-pipeline",
    "title": "Run a Large Language Model using the HuggingFace Transformers API.",
    "section": "First, a Pipeline",
    "text": "First, a Pipeline\nA HuggingFace model is based on 3 key pieces: 1. Config file.\n2. Preprocessor file.\n3. Model file.\nThe HuggingFace API gives us a way of automatically using these pieces directly: the pipeline.\nLet‚Äôs get right it and create a Sentiment Analysis pipeline.\n\n# load in the pipeline object from huggingface\nfrom transformers import pipeline\n\n# create the sentiment analysis pipeline\nclassifier = pipeline(\"sentiment-analysis\")\n\n/Users/cck/mambaforge/envs/llm_base/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nNo model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nDownloading (‚Ä¶)lve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 629/629 [00:00&lt;00:00, 1.06MB/s]\nDownloading model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 268M/268M [00:34&lt;00:00, 7.76MB/s] \nDownloading (‚Ä¶)okenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48.0/48.0 [00:00&lt;00:00, 432kB/s]\nDownloading (‚Ä¶)solve/main/vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232k/232k [00:00&lt;00:00, 9.00MB/s]\n\n\nWe can see in the output message above that HuggingFace automatically picked a decent, default model for us since we didn‚Äôt specify one. Specifically, it chose a distilbert model.\nWe will learn more about what exactly distilbert is and how it works later on. For now, think of it as a useful NLP genie who can tell us how it feels about a given sentence.\n\n# example from the HuggingFace tutorial\nclassifier(\"We are very happy to show you the ü§ó Transformers library.\")\n\n[{'label': 'POSITIVE', 'score': 0.9997795224189758}]\n\n\n\n# passing in several sentences at once, inside a python list\nresults = classifier([\n    \"We are very happy to show you the ü§ó Transformers library.\",\n    \"We hope you don't hate it.\",\n    \"I love Fractal! I'm so glad it's not a cult!\", \n])\n\n# print the output of each results\nfor result in results:\n    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n\nlabel: POSITIVE, with score: 0.9998\nlabel: NEGATIVE, with score: 0.5309\nlabel: POSITIVE, with score: 0.999"
  },
  {
    "objectID": "nbs/index.html",
    "href": "nbs/index.html",
    "title": "Fractal LLMs Course",
    "section": "",
    "text": "::: {.cell 0=‚Äòh‚Äô 1=‚Äòi‚Äô 2=‚Äòd‚Äô 3=‚Äòe‚Äô}\n:::"
  },
  {
    "objectID": "nbs/index.html#install",
    "href": "nbs/index.html#install",
    "title": "Fractal LLMs Course",
    "section": "Install",
    "text": "Install\npip install Fractal_LLM_Course"
  },
  {
    "objectID": "nbs/index.html#how-to-use",
    "href": "nbs/index.html#how-to-use",
    "title": "Fractal LLMs Course",
    "section": "How to use",
    "text": "How to use\nThere is a folder for each lesson inside Fractal_LLM_Course/."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Fractal-U LLM Blog",
    "section": "",
    "text": "No matching items"
  }
]