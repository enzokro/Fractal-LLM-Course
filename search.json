[
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Environment setups",
    "section": "",
    "text": "The goal of this course is to build an AI Agent by fine-tuning a Large Language Model (LLM) on documents chosen by each student. What exactly do we mean by Agent, and why would we focus on a simple one?\nTo give some context, there is a lot of talk and hype around Agents at the moment. Folks are looking towards a future where we all have personalized AI assistants, aka Agents, at our fingertips. These Agents will make our lives better both in the day to day and in the long term. They are the AI assistants of Science Fiction made material. Agents like TARS from Interstellar, Jarvis from Iron Man, HAL 9000 from Space Odyssey, Samantha from Her, etc.\nFor as fast as the progress in AI and LLMs has been, Agents as powerful as those are still a ways off. It’s hard if not impossible to predict the exact timelines. Suffice it to say that these Agents won’t be here anytime “soon”. But, barring some force majeure, they will exist at some point.\nThe gap, then, is that folks are talking about (and promising) these advanced Agent capabilities, while on the ground we are still dealing with LLM hallucinations and prohibitive compute requirements.\nWhere does that leave us? Well, as a recent announcement from OpenAI shows, fine-tuning GPT-3.5 on small, clean datasets can even surpass GPT-4 on certain tasks. That’s what we are aiming for. In other words, we already have the ability to develop outrageously powerful tools by fine-tuning LLMs on small, clean datasets.\nSo we won’t build a simple chatbot, nor will we be build Iron Man’s Jarvis. We will land somewhere in the middle. If it helps, try thinking about this simple Agent as an Intelligent Rubber Duck. In case you’re not familiar with the concept: a Rubber Duck is anything (actual yellow rubber duckie optional) that you keep around your desk and talk to about your work. It is a physical tool for thought, since it’s so often helpful to say out loud the swarm of thoughts in our head.\nOur simple Agent will be a Rubber Duck that speaks back at you. When you ask it a question about your work, it will respond given what it knows about the project as a whole. Or, if you are simply verbalizing a thought to untangle it, the Agent will give you some feedback or suggest other approaches. If we can be so bold: our Agent will be a Jarvis-lite, laser-focused on a narrow scope. Then, as both the tools and tech progresses, we’ll have everything needed to unlock even more capabilities from our Intelligent Rubber Duck.\n\nSummary: In this course we will fine-tune an LLM on a small, clean dataset of our choosing to build an Intelligent Rubber Duck that can help us work or create better.\n\n\n\nIn order to fully use a current, open source LLM, the first thing we need to do is set up a proper programming environment. The environment is a computing ecosystem with all the software libraries and packages needed to run an LLM.\nNote that setting up this environment is often one of the most time-consuming and challenging tasks when it comes to Machine Learning. There is no silver bullet or universal solution, as you will see by the dozens of tools that folks have come up with to tackle this problem (insert xkcd comic about competing standards).\nThe main point here is that setting up the environment is often annoying. It can even be straight up painful. It’s ok to feel lost or struggle with it. Please take some comfort in the fact that once we have the environment, many of the downstream tasks will feel easy by comparison!\nSo what makes building this environment so challenging? And why do we need it in the first place?\n\n\nLLMs, and Machine Learning models more generally, often fail in different ways than other, standard software fails. For instance, classic bugs in regular software include: type mismatches, syntax errors, compilation errors, etc. In other words failures that clearly stem from a wrong operation (aka a bug) that snuck into the code. We wanted the computer to do X, but we told it by accident to do Y instead.\nIn contrast, ML models often have “silent” failures. There is no syntax or compilation error - the program still runs and completes fine. But, there is something wrong in the code: adding where we should have subtracted, grabbing the wrong element from a list, or using the wrong mathematical function. There is no type checker or compiler that would (or even could, for now) catch these errors.\nThe fix for the silent failures above is clear:\n- Carefully inspecting the code.\n- Monitoring and validating its outputs.\n- Clarity in both the algorithms and models we are using.\nThere is another, unfortunate kind of silent failure: version mismatches. Version failures happen when we use a different version of a programming library than the version originally used by the model. As the software libraries we rely on are frequently updated, both subtle and major changes in their internals can affect the output of a model. These failures are unfortunately immune to our careful logical checks.\nAvoiding these silent failures is the main reason for being consistent and disciplined with our model’s programming environment. A good environment setup keeps us focused on the important, conceptual part of our model instead of getting bogged down in managing software versions.\n\n\n\nThere is a nice benefit to spending this much time and effort up front on our environment.\nWe will not only have a specialized environment to run and fine-tune a single LLM. We’ll have a springboard and setup to keep up with the state of the art in the field. A setup to bring in groundbreaking improvements as they are released. And to weave in the latest and greatest models. The LLM world is our oyster, and the base environment the grain of sand soon-to-be pearls.\n\n\n\n\nThe mamba package manager will handle the python version. Why Mamba? To start it is way fast and better than Anaconda, and it makes it easier to install OS and system-level packages we need outside of python.\nWe will use pip to install the actual python packages. Note that we could use mamba for this as well, but a few of the libraries need custom pip options to install.\n\nNote: Run pip install -e . to install a dynamic version of this package that tracks live code changes."
  },
  {
    "objectID": "setup.html#things-we-need-for-the-class",
    "href": "setup.html#things-we-need-for-the-class",
    "title": "Environment setups",
    "section": "",
    "text": "In order to fully use a current, open source LLM, the first thing we need to do is set up a proper programming environment. The environment is a computing ecosystem with all the software libraries and packages needed to run an LLM.\nNote that setting up this environment is often one of the most time-consuming and challenging tasks when it comes to Machine Learning. There is no silver bullet or universal solution, as you will see by the dozens of tools that folks have come up with to tackle this problem (insert xkcd comic about competing standards).\nThe main point here is that setting up the environment is often annoying. It can even be straight up painful. It’s ok to feel lost or struggle with it. Please take some comfort in the fact that once we have the environment, many of the downstream tasks will feel easy by comparison!\nSo what makes building this environment so challenging? And why do we need it in the first place?\n\n\nLLMs, and Machine Learning models more generally, often fail in different ways than other, standard software fails. For instance, classic bugs in regular software include: type mismatches, syntax errors, compilation errors, etc. In other words failures that clearly stem from a wrong operation (aka a bug) that snuck into the code. We wanted the computer to do X, but we told it by accident to do Y instead.\nIn contrast, ML models often have “silent” failures. There is no syntax or compilation error - the program still runs and completes fine. But, there is something wrong in the code: adding where we should have subtracted, grabbing the wrong element from a list, or using the wrong mathematical function. There is no type checker or compiler that would (or even could, for now) catch these errors.\nThe fix for the silent failures above is clear:\n- Carefully inspecting the code.\n- Monitoring and validating its outputs.\n- Clarity in both the algorithms and models we are using.\nThere is another, unfortunate kind of silent failure: version mismatches. Version failures happen when we use a different version of a programming library than the version originally used by the model. As the software libraries we rely on are frequently updated, both subtle and major changes in their internals can affect the output of a model. These failures are unfortunately immune to our careful logical checks.\nAvoiding these silent failures is the main reason for being consistent and disciplined with our model’s programming environment. A good environment setup keeps us focused on the important, conceptual part of our model instead of getting bogged down in managing software versions.\n\n\n\nThere is a nice benefit to spending this much time and effort up front on our environment.\nWe will not only have a specialized environment to run and fine-tune a single LLM. We’ll have a springboard and setup to keep up with the state of the art in the field. A setup to bring in groundbreaking improvements as they are released. And to weave in the latest and greatest models. The LLM world is our oyster, and the base environment the grain of sand soon-to-be pearls."
  },
  {
    "objectID": "setup.html#organizing-what-we-need",
    "href": "setup.html#organizing-what-we-need",
    "title": "Environment setups",
    "section": "",
    "text": "The mamba package manager will handle the python version. Why Mamba? To start it is way fast and better than Anaconda, and it makes it easier to install OS and system-level packages we need outside of python.\nWe will use pip to install the actual python packages. Note that we could use mamba for this as well, but a few of the libraries need custom pip options to install.\n\nNote: Run pip install -e . to install a dynamic version of this package that tracks live code changes."
  },
  {
    "objectID": "setup.html#mac-installation",
    "href": "setup.html#mac-installation",
    "title": "Environment setups",
    "section": "Mac Installation",
    "text": "Mac Installation\nFirst find the name of your architecture. We then use it to pick the right install script for each Mac.\n\n# check your mac's architecture\narch=$(uname) \necho $arch\n\n# download the appropriate installation script\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\"\n\n# run the Mambaforge installer\nbash Mambaforge-$(uname)-$(uname -m).sh\nIf you prefer to download the file directly, grab it from here:\nhttps://github.com/conda-forge/miniforge/releases/"
  },
  {
    "objectID": "setup.html#creating-the-environment",
    "href": "setup.html#creating-the-environment",
    "title": "Environment setups",
    "section": "Creating the environment",
    "text": "Creating the environment\nAfter installing Mamba, head to the Lesson 0 here: Fractal_LLM_Course/lesson_0/envs. The README.md in that folder has the full instructions to build the mamba environment."
  },
  {
    "objectID": "first_runs.html",
    "href": "first_runs.html",
    "title": "Environment setups",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fractal LLMs Course",
    "section": "",
    "text": "pip install Fractal_LLM_Course"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Fractal LLMs Course",
    "section": "",
    "text": "pip install Fractal_LLM_Course"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "Fractal LLMs Course",
    "section": "How to use",
    "text": "How to use\nThere is a folder for each lesson inside Fractal_LLM_Course/."
  }
]